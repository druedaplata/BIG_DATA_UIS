{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs:\n",
    "[tdidf post](https://marcellodesales.wordpress.com/2009/12/31/tf-idf-in-hadoop-part-1-word-frequency-in-doc/)\n",
    "[google](https://code.google.com/p/hadoop-clusternet/wiki/RunningMapReduceExampleTFIDF)\n",
    "[mr-job implementation](https://github.com/louismullie/tf-idf-emr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Job 1: Word Frequency in Doc\n",
    "\n",
    "As mentioned before, the word frequency phase is designed in a Job whose task is to count the number of words in each of the documents in the input directory. In this case, the specification of the Map and Reduce are as follows:\n",
    "\n",
    "    Map:\n",
    "        Input: (document, each line contents)\n",
    "        Output: (word@document, 1))\n",
    "    Reducer\n",
    "        n = sum of the values of for each key “word@document”\n",
    "        Output: ((word@document), n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Job 2: Word Counts for Docs\n",
    "\n",
    "The goal of this job is to count the total number of words for each document, in a way to compare each word with the total number of words. I’ve tried to implement a default InputFormat and I couldn’t find examples related to it. As I understood, the values could be read in the same format they are saved (Text, IntWritable), but I will keep it simple and use the same default InputFormat as before. Following the same definition as in part one, the specification of the Map and Reduce are as follows:\n",
    "\n",
    "    Map:\n",
    "        Input: ((word@document), n)\n",
    "        Re-arrange the mapper to have the key based on each document\n",
    "        Output: (document, word=n)\n",
    "    Reducer\n",
    "        N = totalWordsInDoc = sum [word=n]) for each document\n",
    "        Output: ((word@document), (n/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Job 3: Documents in Corpus and TF-IDF computation\n",
    "   \n",
    "The output shows each term at each document, and the number of its occurrence on the given document, accompanied by the total number of terms in the document. So, the final Mapper and Reducer were defined as follows:\n",
    "\n",
    "    Map:\n",
    "        Input: ((term@document), n/N)\n",
    "        Output: (term, document=n/N)\n",
    "    Reducer:\n",
    "        D = total number of documents in corpus. This can be passed by the driver as a constant;\n",
    "        d = number of documents in corpus where the term appears. \n",
    "        TFIDF = n/N * log(D/d);\n",
    "        Output: ((word@document), d/D, (n/N), TFIDF)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
